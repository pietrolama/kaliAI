#!/usr/bin/env python3
"""
Exploit Hunter - Sistema per analizzare e indicizzare fonti di exploit/vulnerabilitÃ .

Fonti supportate:
- Exploit-DB (API + scraping)
- GitHub (API search per PoC)
- Packet Storm (RSS)
- NVD (API)
- Google Project Zero (scraping blog)
- Reddit (RSS feeds)
- Security vendor blogs (RSS)
"""

import os
import sys
import json
import logging
import requests
import feedparser
import re
from datetime import datetime, timedelta
from pathlib import Path
from typing import List, Dict, Optional
from bs4 import BeautifulSoup
import time

# Aggiungi parent directory al path per import
sys.path.insert(0, os.path.dirname(os.path.dirname(__file__)))
from knowledge.knowledge_enhancer import knowledge_enhancer

logger = logging.getLogger('ExploitHunter')


class ExploitHunter:
    """Cacciatore di exploit da fonti multiple."""
    
    def __init__(self, cache_dir: str = "data/exploit_cache"):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36'
        })
        
        # GitHub token (opzionale, per rate limit piÃ¹ alti)
        self.github_token = os.getenv('GITHUB_TOKEN', '')
        if self.github_token:
            self.session.headers['Authorization'] = f'token {self.github_token}'
    
    def search_github_pocs(self, query: str, max_results: int = 10) -> List[Dict]:
        """
        Cerca PoC su GitHub.
        
        Query examples:
        - "CVE-2024 PoC"
        - "exploit apache"
        - "IoT vulnerability proof-of-concept"
        """
        logger.info(f"ðŸ” GitHub search: {query}")
        
        try:
            url = "https://api.github.com/search/repositories"
            params = {
                'q': query,
                'sort': 'updated',
                'order': 'desc',
                'per_page': max_results
            }
            
            response = self.session.get(url, params=params, timeout=15)
            
            if response.status_code == 403:
                logger.warning("âš ï¸  GitHub rate limit raggiunto")
                return []
            
            response.raise_for_status()
            data = response.json()
            
            results = []
            for item in data.get('items', []):
                results.append({
                    'source': 'github',
                    'title': item.get('name', ''),
                    'description': item.get('description', ''),
                    'url': item.get('html_url', ''),
                    'stars': item.get('stargazers_count', 0),
                    'updated': item.get('updated_at', ''),
                    'language': item.get('language', '')
                })
            
            logger.info(f"  âœ… Trovati {len(results)} repository")
            return results
            
        except Exception as e:
            logger.error(f"âŒ Errore GitHub search: {e}")
            return []
    
    def scrape_exploitdb_recent(self, days: int = 7, max_results: int = 20) -> List[Dict]:
        """
        Scrape exploit recenti da Exploit-DB.
        
        Nota: Exploit-DB ha anche API, ma il scraping Ã¨ piÃ¹ semplice.
        """
        logger.info(f"ðŸ” Exploit-DB: ultimi {days} giorni")
        
        try:
            url = "https://www.exploit-db.com/"
            response = self.session.get(url, timeout=15)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # Trova tabella exploit
            results = []
            table = soup.find('table', {'id': 'exploits-table'})
            
            if not table:
                logger.warning("âš ï¸  Tabella exploit non trovata (layout cambiato?)")
                return []
            
            rows = table.find_all('tr')[:max_results]
            
            for row in rows[1:]:  # Skip header
                cols = row.find_all('td')
                if len(cols) >= 4:
                    results.append({
                        'source': 'exploitdb',
                        'title': cols[4].get_text(strip=True) if len(cols) > 4 else '',
                        'type': cols[3].get_text(strip=True) if len(cols) > 3 else '',
                        'platform': cols[5].get_text(strip=True) if len(cols) > 5 else '',
                        'date': cols[2].get_text(strip=True) if len(cols) > 2 else '',
                        'url': f"https://www.exploit-db.com{cols[4].find('a')['href']}" if len(cols) > 4 and cols[4].find('a') else ''
                    })
            
            logger.info(f"  âœ… Trovati {len(results)} exploit")
            return results
            
        except Exception as e:
            logger.error(f"âŒ Errore Exploit-DB scraping: {e}")
            return []
    
    def fetch_packetstorm_rss(self, max_items: int = 20) -> List[Dict]:
        """Fetcha RSS feed di Packet Storm Security."""
        logger.info("ðŸ” Packet Storm RSS feed")
        
        try:
            feed = feedparser.parse('https://rss.packetstormsecurity.com/')
            
            results = []
            for entry in feed.entries[:max_items]:
                results.append({
                    'source': 'packetstorm',
                    'title': entry.get('title', ''),
                    'description': entry.get('summary', ''),
                    'url': entry.get('link', ''),
                    'date': entry.get('published', '')
                })
            
            logger.info(f"  âœ… Trovati {len(results)} items")
            return results
            
        except Exception as e:
            logger.error(f"âŒ Errore Packet Storm RSS: {e}")
            return []
    
    def scrape_google_project_zero(self, max_posts: int = 10) -> List[Dict]:
        """Scrape blog Google Project Zero."""
        logger.info("ðŸ” Google Project Zero blog")
        
        try:
            url = "https://googleprojectzero.blogspot.com/"
            response = self.session.get(url, timeout=15)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            results = []
            posts = soup.find_all('article', limit=max_posts)
            
            for post in posts:
                title_elem = post.find('h3', class_='post-title')
                content_elem = post.find('div', class_='post-body')
                
                if title_elem:
                    title = title_elem.get_text(strip=True)
                    url_elem = title_elem.find('a')
                    post_url = url_elem['href'] if url_elem else ''
                    content = content_elem.get_text(strip=True)[:500] if content_elem else ''
                    
                    results.append({
                        'source': 'google_project_zero',
                        'title': title,
                        'content': content,
                        'url': post_url
                    })
            
            logger.info(f"  âœ… Trovati {len(results)} post")
            return results
            
        except Exception as e:
            logger.error(f"âŒ Errore Google Project Zero: {e}")
            return []
    
    def fetch_reddit_exploitdev(self, max_posts: int = 20) -> List[Dict]:
        """Fetcha subreddit ExploitDev."""
        logger.info("ðŸ” Reddit r/ExploitDev")
        
        try:
            feed = feedparser.parse('https://www.reddit.com/r/ExploitDev/.rss')
            
            results = []
            for entry in feed.entries[:max_posts]:
                results.append({
                    'source': 'reddit_exploitdev',
                    'title': entry.get('title', ''),
                    'description': entry.get('summary', ''),
                    'url': entry.get('link', ''),
                    'date': entry.get('published', '')
                })
            
            logger.info(f"  âœ… Trovati {len(results)} post")
            return results
            
        except Exception as e:
            logger.error(f"âŒ Errore Reddit: {e}")
            return []
    
    def fetch_nvd_recent_cves(self, days: int = 7, max_results: int = 50) -> List[Dict]:
        """
        Fetcha CVE recenti da NVD (National Vulnerability Database).
        
        API: https://nvd.nist.gov/developers/vulnerabilities
        """
        logger.info(f"ðŸ” NVD: CVE ultimi {days} giorni")
        
        try:
            # Calcola date range
            end_date = datetime.now()
            start_date = end_date - timedelta(days=days)
            
            url = "https://services.nvd.nist.gov/rest/json/cves/2.0"
            params = {
                'pubStartDate': start_date.strftime('%Y-%m-%dT00:00:00.000'),
                'pubEndDate': end_date.strftime('%Y-%m-%dT23:59:59.999'),
                'resultsPerPage': max_results
            }
            
            response = self.session.get(url, params=params, timeout=30)
            response.raise_for_status()
            
            data = response.json()
            
            results = []
            for vuln in data.get('vulnerabilities', []):
                cve = vuln.get('cve', {})
                cve_id = cve.get('id', '')
                
                descriptions = cve.get('descriptions', [])
                description = descriptions[0].get('value', '') if descriptions else ''
                
                results.append({
                    'source': 'nvd',
                    'cve_id': cve_id,
                    'description': description,
                    'published': cve.get('published', ''),
                    'severity': self._extract_severity(cve),
                    'url': f"https://nvd.nist.gov/vuln/detail/{cve_id}"
                })
            
            logger.info(f"  âœ… Trovati {len(results)} CVE")
            return results
            
        except Exception as e:
            logger.error(f"âŒ Errore NVD API: {e}")
            return []
    
    def _extract_severity(self, cve_data: Dict) -> str:
        """Estrae severity da dati CVE."""
        metrics = cve_data.get('metrics', {})
        
        # CVSSv3
        cvss_v3 = metrics.get('cvssMetricV31', [])
        if cvss_v3:
            return cvss_v3[0].get('cvssData', {}).get('baseSeverity', 'UNKNOWN')
        
        # CVSSv2 fallback
        cvss_v2 = metrics.get('cvssMetricV2', [])
        if cvss_v2:
            score = cvss_v2[0].get('cvssData', {}).get('baseScore', 0)
            if score >= 7.0:
                return 'HIGH'
            elif score >= 4.0:
                return 'MEDIUM'
            else:
                return 'LOW'
        
        return 'UNKNOWN'
    
    def index_results(self, results: List[Dict], category: str):
        """Indicizza risultati nel ChromaDB."""
        if not results:
            return 0
        
        indexed = 0
        for res in results:
            try:
                source = res.get('source', category)
                
                # Crea documento testuale
                doc_parts = [f"SOURCE: {source}"]
                
                for key, value in res.items():
                    if key != 'source' and value:
                        doc_parts.append(f"{key.upper()}: {value}")
                
                doc_text = '\n'.join(doc_parts)
                
                # Genera ID unico
                doc_id = f"{source}_{hash(res.get('url', '') or res.get('title', ''))}"
                
                # Aggiungi a collection appropriata
                if 'cve' in source or 'nvd' in source:
                    collection = knowledge_enhancer.cve_collection
                elif 'exploit' in source:
                    collection = knowledge_enhancer.exploits_collection
                else:
                    collection = knowledge_enhancer.kb_collection
                
                collection.add(
                    documents=[doc_text],
                    metadatas=[{
                        'source': source,
                        'category': category,
                        'timestamp': datetime.now().isoformat()
                    }],
                    ids=[doc_id]
                )
                
                indexed += 1
                
            except Exception as e:
                logger.warning(f"âš ï¸  Errore indicizzazione: {e}")
                continue
        
        logger.info(f"  ðŸ“¦ Indicizzati {indexed}/{len(results)} documenti")
        return indexed
    
    def hunt_all(self, github_queries: List[str] = None, nvd_days: int = 7):
        """
        Esegue hunting completo da tutte le fonti.
        
        Args:
            github_queries: Lista di query GitHub (es. ["CVE-2024 PoC", "IoT exploit"])
            nvd_days: Giorni di CVE da fetchare da NVD
        """
        logger.info("ðŸŽ¯ AVVIO EXPLOIT HUNTING")
        logger.info("=" * 70)
        
        stats = {
            'github': 0,
            'exploitdb': 0,
            'packetstorm': 0,
            'google_p0': 0,
            'reddit': 0,
            'nvd': 0
        }
        
        # 1. GitHub PoC search
        if github_queries:
            for query in github_queries:
                results = self.search_github_pocs(query, max_results=5)
                stats['github'] += self.index_results(results, 'github_poc')
                time.sleep(2)  # Rate limiting
        
        # 2. Exploit-DB
        results = self.scrape_exploitdb_recent(days=7, max_results=15)
        stats['exploitdb'] += self.index_results(results, 'exploitdb')
        time.sleep(2)
        
        # 3. Packet Storm
        results = self.fetch_packetstorm_rss(max_items=15)
        stats['packetstorm'] += self.index_results(results, 'packetstorm')
        time.sleep(2)
        
        # 4. Google Project Zero
        results = self.scrape_google_project_zero(max_posts=5)
        stats['google_p0'] += self.index_results(results, 'google_project_zero')
        time.sleep(2)
        
        # 5. Reddit
        results = self.fetch_reddit_exploitdev(max_posts=10)
        stats['reddit'] += self.index_results(results, 'reddit')
        time.sleep(2)
        
        # 6. NVD CVEs
        results = self.fetch_nvd_recent_cves(days=nvd_days, max_results=30)
        stats['nvd'] += self.index_results(results, 'nvd_cve')
        
        logger.info("=" * 70)
        logger.info("âœ… HUNTING COMPLETATO")
        
        return stats


# Istanza globale
exploit_hunter = ExploitHunter()


if __name__ == "__main__":
    logging.basicConfig(
        level=logging.INFO,
        format='[%(levelname)s] %(message)s'
    )
    
    print("=" * 70)
    print("ðŸŽ¯ EXPLOIT HUNTER - Sistema di Analisi Fonti")
    print("=" * 70)
    print()
    
    # Stats iniziali
    stats_before = knowledge_enhancer.get_stats()
    print("ðŸ“Š Knowledge Base - Prima:")
    print(f"  Total: {stats_before['total']} documenti")
    print()
    
    # Definisci query GitHub interessanti
    github_queries = [
        "CVE-2024 PoC",
        "CVE-2023 exploit",
        "IoT vulnerability proof-of-concept",
        "router exploit",
        "camera vulnerability"
    ]
    
    print("ðŸ” Query GitHub da cercare:")
    for q in github_queries:
        print(f"  - {q}")
    print()
    
    # Esegui hunting
    print("ðŸš€ Avvio hunting...")
    print()
    
    hunting_stats = exploit_hunter.hunt_all(
        github_queries=github_queries,
        nvd_days=7
    )
    
    # Stats finali
    print()
    print("=" * 70)
    print("ðŸ“Š RISULTATI:")
    print("=" * 70)
    
    for source, count in hunting_stats.items():
        if count > 0:
            print(f"  âœ… {source}: {count} documenti indicizzati")
    
    stats_after = knowledge_enhancer.get_stats()
    total_added = stats_after['total'] - stats_before['total']
    
    print()
    print(f"ðŸ“¦ Knowledge Base - Dopo: {stats_after['total']} (+{total_added})")
    print()
    print("=" * 70)
    print("ðŸŽ‰ COMPLETATO!")
    print("=" * 70)

